{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Tausal21/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/Tausal21/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[0.934  0.9172 0.9224 0.918  0.9136 0.922  0.912  0.9332 0.924  0.9236]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer #not used due to poor performance\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def construct_file_list(directory):\n",
    "\tfile_list = []\n",
    "\tfor text_file in sorted(os.listdir(directory)):\n",
    "\t\tos.chdir(directory)\n",
    "\t\tif text_file.endswith(\".txt\"):\n",
    "\t    \t#print(os.getcwd())\n",
    "\t\t\tfile_list.append(text_file)\n",
    "\tif(directory == '/Users/Tausal21/Desktop/comp_551/mini_peoject_02/test'):\n",
    "\t\t#file_list.sort(key=lambda f: int(filter(str.isdigit, f)))\n",
    "\t\tfile_list = sorted(file_list , key=lambda x: int(os.path.splitext(x)[0]))\n",
    "\treturn file_list\n",
    "\n",
    "def construct_feature_matrix_countVectorizer(directory, maximum_features, target_column_value, boolean_train):\n",
    "\tfile_list = construct_file_list(directory)\n",
    "\n",
    "\tcv = CountVectorizer(input='filename', stop_words='english', max_df=0.75, min_df=5, max_features=maximum_features)\n",
    "\tvec = cv.fit(file_list)\n",
    "\n",
    "\tbag_of_words = vec.transform(file_list)\n",
    "\tfeature_matrix_withoutTarget = pd.DataFrame(bag_of_words.toarray(), columns=vec.get_feature_names())\n",
    "\n",
    "\tif(boolean_train == False):\n",
    "\t\tfile_list_df = pd.DataFrame({'col': file_list})\n",
    "\t\t#print(file_list_df)\n",
    "\t\treturn [feature_matrix_withoutTarget, file_list_df] #This returns the test_set feature matrix\n",
    "\telse:\n",
    "\t\tif(target_column_value == 1):\n",
    "\t\t\tY_raw = np.ones((12500,), dtype=float)\n",
    "\t\t\ttarget_column_Y = pd.DataFrame(Y_raw, columns=['TARGET_Y']) #appends target column for positive review\n",
    "\t\telse:\n",
    "\t\t\tY_raw = np.zeros((12500,), dtype=float)\n",
    "\t\t\ttarget_column_Y = pd.DataFrame(Y_raw, columns=['TARGET_Y']) #appends target column for positive review\n",
    "\t\t\n",
    "\t\tfeature_matrix = pd.concat([feature_matrix_withoutTarget, target_column_Y], axis=1)\n",
    "\t\treturn feature_matrix #returns pos/neg training feature matrix with labels\n",
    "\n",
    "def kFold_LogisticRegression(feature_matrix_input, no_of_folds):\n",
    "\tmodel = LogisticRegression()\n",
    "\tscores = cross_val_score(model, feature_matrix_input.loc[:, feature_matrix_input.columns != 'TARGET_Y'], feature_matrix_input.TARGET_Y, cv=no_of_folds)\n",
    "\treturn scores\n",
    "\n",
    "def Logistic_reg(train_X, train_Y, test_X):\n",
    "\tprint(\"train_X shape: \", train_X.shape)\n",
    "\tprint(\"train_Y shape: \", train_Y.shape)\n",
    "\tmodel = LogisticRegression()\n",
    "\tmodel.fit(train_X, train_Y)\n",
    "\n",
    "\tprediction_list = model.predict(test_X)\n",
    "\treturn prediction_list\n",
    "\n",
    "\n",
    "\n",
    "pos_dir = '/Users/Tausal21/Desktop/comp_551/mini_peoject_02/train/pos'\n",
    "neg_dir = '/Users/Tausal21/Desktop/comp_551/mini_peoject_02/train/neg'\n",
    "test_dir = '/Users/Tausal21/Desktop/comp_551/mini_peoject_02/test'\n",
    "\n",
    "pos_features = construct_feature_matrix_countVectorizer(pos_dir, 5000, 1, True)\n",
    "print(\"Pos_features shape: \", pos_features.shape)\n",
    "neg_features = construct_feature_matrix_countVectorizer(neg_dir, 5000, 0, True)\n",
    "print(\"Neg_features shape: \", neg_features.shape)\n",
    "\n",
    "feature_matrix_list = [pos_features, neg_features]\n",
    "feature_matrix_final = pd.concat(feature_matrix_list, sort=False).fillna(0)\n",
    "print(\"final_feature shape: \", feature_matrix_final.shape)\n",
    "no_of_features = len(feature_matrix_final.columns) - 1\n",
    "print(\"number of features: \", no_of_features)\n",
    "\n",
    "[test_set, file_list_df] = construct_feature_matrix_countVectorizer(test_dir, no_of_features, 0, False)\n",
    "print(\"test_set shape: \", test_set.shape)\n",
    "print(file_list_df)\n",
    "\n",
    "#scores = kFold_LogisticRegression(feature_matrix_final, 10)\n",
    "prediction_list = Logistic_reg(feature_matrix_final.loc[:, feature_matrix_final.columns != 'TARGET_Y'], feature_matrix_final.TARGET_Y, test_set)\n",
    "prediction_list_df = pd.DataFrame(prediction_list)\n",
    "\n",
    "\n",
    "output_df = pd.concat([file_list_df, prediction_list_df], axis=1)\n",
    "output_df.to_csv('output2.csv')\n",
    "print(output_df)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport pandas as pd\\nimport nltk\\nfrom bs4 import BeautifulSoup \\nnltk.download(\\'stopwords\\')\\nnltk.download(\\'wordnet\\')\\nfrom nltk.stem import PorterStemmer #not used due to poor performance\\nfrom nltk.stem import WordNetLemmatizer  \\nfrom nltk.corpus import stopwords \\nfrom nltk.tokenize import word_tokenize \\nfrom nltk.corpus.reader.plaintext import PlaintextCorpusReader\\nfrom sklearn.feature_extraction.text import CountVectorizer\\n\\n\\ncorpus = [\\n    \"This is my house My house is beautiful just next to McGill\",\\n    \"coming back to life\",\\n    \"McGill University was founded in 1821\"\\n]\\n\\n\\nfile_list2 = []\\ndir = \\'/Users/Tausal21/Desktop/551_rough\\'\\nfor text_file in os.listdir(dir):\\n    os.chdir(dir)\\n    if text_file.endswith(\".txt\"):\\n        #print(os.getcwd())\\n        file_list2.append(text_file)\\n\\ncv = CountVectorizer(input=\\'filename\\', stop_words=\\'english\\')\\nvec = cv.fit(file_list2)\\n#print(vec)\\nbag_of_words = vec.transform(file_list2)\\n#print(type(bag_of_words))\\n\\nfeature_X = pd.DataFrame(bag_of_words.toarray(), columns=vec.get_feature_names() )\\nfeature_X.head()\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer #not used due to poor performance\n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "corpus = [\n",
    "    \"This is my house My house is beautiful just next to McGill\",\n",
    "    \"coming back to life\",\n",
    "    \"McGill University was founded in 1821\"\n",
    "]\n",
    "\n",
    "\n",
    "file_list2 = []\n",
    "dir = '/Users/Tausal21/Desktop/551_rough'\n",
    "for text_file in os.listdir(dir):\n",
    "    os.chdir(dir)\n",
    "    if text_file.endswith(\".txt\"):\n",
    "        #print(os.getcwd())\n",
    "        file_list2.append(text_file)\n",
    "\n",
    "cv = CountVectorizer(input='filename', stop_words='english')\n",
    "vec = cv.fit(file_list2)\n",
    "#print(vec)\n",
    "bag_of_words = vec.transform(file_list2)\n",
    "#print(type(bag_of_words))\n",
    "\n",
    "feature_X = pd.DataFrame(bag_of_words.toarray(), columns=vec.get_feature_names() )\n",
    "feature_X.head()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
